{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLDaTo\n",
    "\n",
    "> API details. MLDaTo stand for machine learning on any data in an automated fasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from nbdev.showdoc import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('D:/Work/R Packages/Testing data/CreditModellingTestCase.csv',sep=';')\n",
    "df = df.loc[df['default'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "* Depends: pandas, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_df(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    A more advanced version of describe for tabular exploratory data analysis. Inlcudes additional information such as,\n",
    "    missing observations, unique observations, constant feature flagging, all_missing feature flagging, feature types & outlier\n",
    "    values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas df, required, default=NA\n",
    "        Pandas dataframe object \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas df\n",
    "        Returns a pandas dataframe object\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "        df = pd.DataFrame({\"x1\": [\"a\", \"b\", \"c\", \"a\"], \"x2\":['x','y','x','x'], \"y\": [1,1,0,1]})\n",
    "        eda = explore_df(df=df)\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    ft = pd.DataFrame()\n",
    "    ft['type']=df.dtypes.astype(str)\n",
    "    ft['feature']=ft.index\n",
    "    ft['unique']=df.nunique()\n",
    "    ft['missing']= df.isnull().sum()\n",
    "    ft['constant']=np.where(ft['unique']==1,1,0)\n",
    "    ft['all_missing']=np.where(ft['missing']==df.shape[0],1,0)\n",
    "\n",
    "    numeric = ft.loc[(ft['type'].str.contains('float'))]['feature']\n",
    "    numeric = numeric.append(ft.loc[(ft['type'].str.contains('int'))]['feature'])\n",
    "    \n",
    "    categorical = ft.loc[(ft['type'].str.contains('object'))]['feature']\n",
    "\n",
    "    # Summary statistics\n",
    "    lower=df[numeric].quantile(q=0.25)\n",
    "    upper=df[numeric].quantile(q=0.75)\n",
    "    ft['min']=df[numeric].min()\n",
    "    ft['q1']=lower\n",
    "    ft['median']=df[numeric].median()\n",
    "    ft['mean']=df[numeric].mean()\n",
    "    ft['q3']=upper\n",
    "    ft['max']=df[numeric].max()\n",
    "\n",
    "    # Caclulate outlier values\n",
    "    iqr = upper - lower\n",
    "    lower=lower-(1.5*iqr)\n",
    "    upper=upper+(1.5*iqr)\n",
    "    ft['lower_outlier']=lower\n",
    "    ft['upper_outlier']=upper\n",
    "    ft['skewness']=df[numeric].skew()\n",
    "    \n",
    "    ft['class'] = np.where(ft['type'].str.contains('float'), 'numeric', None)\n",
    "    ft['class'] = np.where(ft['type'].str.contains('int'), 'numeric', ft['class'])\n",
    "    ft['class'] = np.where(ft['type'].str.contains('object'), 'categorical', ft['class'])\n",
    "    ft['class'] = np.where(ft['type'].str.contains('datetime'), 'datetime', ft['class'])\n",
    "    ft['class'] = np.where(ft['class'].isin(['numeric','integer']) & \n",
    "                           (ft['min'] == 0) & \n",
    "                           (ft['max'] == 1) & \n",
    "                           (ft['unique'] == 2), 'indicator', ft['class'])\n",
    "        \n",
    "    ft=ft[['feature','type','class','missing','unique','constant','all_missing','min','q1','median',\n",
    "         'mean','q3','max','lower_outlier','upper_outlier','skewness']]\n",
    "\n",
    "    ft=ft.reset_index(drop=True)\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "eda = explore_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format features\n",
    "\n",
    "* Depends: numpy, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_features(df):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies feature formatting to features to comply with machine learning models. Boolean features are transformed to integer \n",
    "    and objects (where applicable) are tried to be formatted as datetime. The resulting feature set should only contain float,\n",
    "    integer, object and datetime type features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas df, required, default=NA\n",
    "        Pandas dataframe object \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas df\n",
    "        Returns a pandas dataframe object\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "        df = pd.DataFrame({\"x1\": [\"a\", \"b\", \"c\", \"a\"], \"x2\":['x','y','x','x'], \"y\": [1,1,0,1]})\n",
    "        df = format_features(df=df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try and convert relevant datetime columns to datetime format\n",
    "    df = df.apply(lambda col: pd.to_datetime(col, errors='ignore') \n",
    "              if col.dtypes == object \n",
    "              else col, \n",
    "              axis=0)\n",
    "\n",
    "    ft = pd.DataFrame()\n",
    "    ft['from_type'] = df.dtypes.astype(str)\n",
    "    ft['feature'] = ft.index\n",
    "    logical = ft.loc[ft['from_type'] == 'bool']['feature'].unique()\n",
    "    df[logical] = df[logical].astype(int) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "df = format_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect features\n",
    "\n",
    "* TODO: Add in functionality to detect flag features, having nr of uniques = 2 and set values\n",
    "* Depends: numpy, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(df, y = None, id = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    \"\"\"\n",
    "    Detects features of different types and returns each type in individual series objects. Feature types detected and grouped are\n",
    "    numeric (float and integer), categorical (object and category), datetime (date) and flags (unique of 2 containing special\n",
    "    values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas df, required, default=NA\n",
    "        Pandas dataframe object \n",
    "    y: str, optional, default=None\n",
    "        Name of the target feature\n",
    "    id: str, optional, default=None\n",
    "        Name of the id feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas series\n",
    "        Returns a series for numeric, categorical, datetime and flag features in that order\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "        df = pd.DataFrame({\"x1\": [\"a\", \"b\", \"c\", \"a\"], \"x2\":['x','y','x','x'], \"y\": [1,1,0,1]})\n",
    "        numeric, categorical, datetime, flag = detect_features(df)\n",
    "    \"\"\"\n",
    "\n",
    "    ft = pd.DataFrame()\n",
    "    ft['type'] = df.dtypes.astype(str)\n",
    "    ft['feature']=ft.index\n",
    "    ft['unique']=df.nunique()\n",
    "    ft.reset_index(drop=True, inplace=True)\n",
    "    numeric = ft.loc[(ft['type'].str.contains('float'))]['feature']\n",
    "    numeric = numeric.append(ft.loc[(ft['type'].str.contains('int'))]['feature'])\n",
    "\n",
    "    categorical = ft.loc[(ft['type'].str.contains('object'))]['feature']\n",
    "    categorical = categorical.append(ft.loc[(ft['type'].str.contains('category'))]['feature'])\n",
    "\n",
    "    datetime = ft.loc[(ft['type'].str.contains('date'))]['feature']\n",
    "\n",
    "    if y is not None:\n",
    "        numeric = numeric.loc[numeric != y]\n",
    "        categorical = categorical.loc[categorical != y]\n",
    "\n",
    "    if id is not None:\n",
    "        numeric = numeric.loc[numeric != id]\n",
    "        categorical = categorical.loc[categorical != id]\n",
    "\n",
    "    return numeric, categorical, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "numeric, categorical, datetime = detect_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parition data\n",
    "\n",
    "Parition data into a train, validation and test set either by stratified random sampling or time sensitive partitioning. For time sensitive partitioning, the data is sorted by the time dependent feature and then split according to percentages of the data running acrros time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'default'\n",
    "x = ['account_amount_added_12_24m',\n",
    "       'account_days_in_dc_12_24m', 'account_days_in_rem_12_24m']\n",
    "test_percentage = 0.2\n",
    "valid_percentage = 0.1\n",
    "time_based_split_feature = None\n",
    "seed = 1234\n",
    "max_uniques = 100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "n_test = df.shape[0] * test_percentage\n",
    "n_valid = df.shape[0] * valid_percentage\n",
    "\n",
    "if time_based_split_feature is not None:\n",
    "    df = df.sort_values(time_based_split_feature)\n",
    "    test = df.tail(round(n_test))\n",
    "    \n",
    "    train = df.iloc[ : df.shape[0] - round(n_test)]\n",
    "    valid = train.tail(round(n_valid))\n",
    "    \n",
    "    train = train.iloc[ : train.shape[0] - round(n_valid)]\n",
    "\n",
    "if y is not None:\n",
    "    \n",
    "    # Classification outcome\n",
    "    if df[y].nunique() <= max_uniques:\n",
    "        \n",
    "        df[y] = df[y].astype('category').cat.codes # Transform target feature\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(df[x], df[y], stratify=df[y], \n",
    "                                                        test_size=test_percentage, random_state=seed)\n",
    "        \n",
    "        train = x_train.copy()\n",
    "        train[y] = y_train\n",
    "        \n",
    "        test = x_test.copy()\n",
    "        test[y] = y_test\n",
    "        \n",
    "        # Caluclate new validation split   \n",
    "        valid_percentage = n_valid / x_train.shape[0]\n",
    "\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(train[x], train[y], stratify=train[y], \n",
    "                                                            test_size=valid_percentage, random_state=seed)\n",
    "\n",
    "        train = x_train.copy()\n",
    "        train[y] = y_train\n",
    "\n",
    "        valid = x_valid.copy()\n",
    "        valid[y] = y_valid\n",
    "    \n",
    "    # Regression\n",
    "    if df[y].nunique() > max_uniques:\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(df[x], df[y], test_size=test_percentage, random_state=seed)\n",
    "        \n",
    "        train = x_train.copy()\n",
    "        train[y] = y_train\n",
    "        \n",
    "        test = x_test.copy()\n",
    "        test[y] = y_test\n",
    "        \n",
    "        # Caluclate new validation split   \n",
    "        valid_percentage = n_valid / x_train.shape[0]\n",
    "\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(train[x], train[y], test_size=valid_percentage, random_state=seed)\n",
    "\n",
    "        train = x_train.copy()\n",
    "        train[y] = y_train\n",
    "\n",
    "        valid = x_valid.copy()\n",
    "        valid[y] = y_valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
